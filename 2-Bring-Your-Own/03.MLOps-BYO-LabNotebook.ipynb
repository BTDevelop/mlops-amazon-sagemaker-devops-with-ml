{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Environment for BYOA Pipeline\n",
    "\n",
    "This notebook instance will act as the lab environment for setting up and triggering changes to our pipeline.  This is being used to provide a consistent environment, gain some familiarity with Amazon SageMaker Notebook Instances, and to avoid any issues with debugging individual laptop configurations during the workshop. \n",
    "\n",
    "PLEASE review the [sample notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb) for detailed documentation on the model being built\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Upload Data to S3 \n",
    "\n",
    "We will utilize this notebook to perform some of the setup that will be required to trigger the first execution of our pipeline.   In this first step in our Machine Learning pipeline, we are going to simulate what would typically be the last step in an Analytics pipeline of creating data science datasets. \n",
    "\n",
    "To accomplish this, we will actually be uploading data from our local notebook instance (data can be found under /data/1-train/*) to S3.  In a typical scenario, this would be done through your analytics pipeline.  We will use the S3 bucket that was created through the CloudFormation template we launched at the beginning of the lab. You can validate the S3 bucket exists by:\n",
    "  1. Going to the [S3 Service](https://s3.console.aws.amazon.com/s3/) inside the AWS Console\n",
    "  2. Find the name of the S3 data bucket created by the CloudFormation template: mlops-data-*yourintials*-*randomid*\n",
    "  \n",
    "In the code cell below, we'll take a look at the training/test/validation datasets and then upload them to S3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Data\n",
      "         setosa  5.1  3.5  1.4  0.2\n",
      "0       setosa  4.9  3.0  1.4  0.2\n",
      "1       setosa  4.7  3.2  1.3  0.2\n",
      "..         ...  ...  ...  ...  ...\n",
      "147  virginica  6.2  3.4  5.4  2.3\n",
      "148  virginica  5.9  3.0  5.1  1.8\n",
      "\n",
      "[149 rows x 5 columns]\n",
      "\n",
      "Smoke Test Data\n",
      "        setosa  5.0  3.5  1.3  0.3\n",
      "0  versicolor  5.5  2.6  4.4  1.2\n",
      "1   virginica  5.8  2.7  5.1  1.9\n",
      "\n",
      "Validation Data\n",
      "         setosa  5.2  3.5  1.5  0.2\n",
      "0       setosa  5.2  3.4  1.4  0.2\n",
      "1       setosa  4.7  3.2  1.6  0.2\n",
      "..         ...  ...  ...  ...  ...\n",
      "42  versicolor  5.9  3.2  4.8  1.8\n",
      "43  versicolor  6.1  2.8  4.0  1.3\n",
      "\n",
      "[44 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('./data/1-train/train/iris.csv', sep=',')\n",
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 5)         # Keep the output on one page\n",
    "print('\\nTraining Data\\n', train_data)\n",
    "\n",
    "smoketest_data = pd.read_csv('./data/1-train/smoketest/iris.csv', sep=',')\n",
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 5)         # Keep the output on one page\n",
    "print('\\nSmoke Test Data\\n', smoketest_data)\n",
    "\n",
    "validation_data = pd.read_csv('./data/1-train/validation/iris.csv', sep=',')\n",
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 5)         # Keep the output on one page\n",
    "print('\\nValidation Data\\n', validation_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UPDATE THE BUCKET NAME BELOW BEFORE EXECUTING THE CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import time\n",
    "\n",
    "# UPDATE THE NAME OF THE BUCKET TO MATCH THE ONE WE CREATED THROUGH THE CLOUDFORMATION TEMPLATE\n",
    "# Example: mlops-data-jdd-df4d4850\n",
    "#bucket = 'mlops-data-<yourinitials>-<generated id>'\n",
    "bucket = 'mlops-data-jdd-ec8a0350'\n",
    "\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "ts = time.strftime(\"%m%d%Y%H%M%S\")\n",
    "trainfilename = 'train/train'+ts+'.csv'\n",
    "smoketestfilename = 'smoketest/smoketest.csv'\n",
    "validationfilename = 'validation/validation.csv'\n",
    "\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "s3.meta.client.upload_file('./data/1-train/train/iris.csv', bucket, trainfilename)\n",
    "s3.meta.client.upload_file('./data/1-train/smoketest/iris.csv', bucket, smoketestfilename)\n",
    "s3.meta.client.upload_file('./data/1-train/validation/iris.csv', bucket, validationfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:  Commit Training Code To Trigger Pipeline Build\n",
    "\n",
    "In this step, we are trigger an execution of the pipeline by committing our training code to the CodeCommit repository that was setup as part of the CloudFormation stack.  The pipeline is currently setup to trigger on a commit to the master branch; however, this should be adjusted in a real-world scenario based on your branching strategy. \n",
    "\n",
    "The CodeCommit repository created can be viewed by:\n",
    "  1. Going to the [CodeCommit Service](https://console.aws.amazon.com/codesuite/codecommit/repositories) inside the AWS Console\n",
    "  2. Find the name of the repository created by the CloudFormation template: mlops-codecommit-byo-*yourinitials*\n",
    "  \n",
    "**UPDATE** Ensure you update the cell below where noted prior to executing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin\tgit@github.com:aws-samples/amazon-sagemaker-devops-with-ml.git (fetch)\n",
      "origin\tgit@github.com:aws-samples/amazon-sagemaker-devops-with-ml.git (push)\n",
      "codecommit\thttps://git-codecommit.us-east-1.amazonaws.com/v1/repos/mlops-codecommit-byo-jdd (fetch)\n",
      "codecommit\thttps://git-codecommit.us-east-1.amazonaws.com/v1/repos/mlops-codecommit-byo-jdd (push)\n",
      "origin\tgit@github.com:aws-samples/amazon-sagemaker-devops-with-ml.git (fetch)\n",
      "origin\tgit@github.com:aws-samples/amazon-sagemaker-devops-with-ml.git (push)\n"
     ]
    }
   ],
   "source": [
    "# View the CodeCommit repository -\n",
    "# This Git integration was configured as part of the creation of the notebook instance in the CloudFormation stack.\n",
    "\n",
    "# The following will return the CodeCommit repository that has been configured with this notebook and will be used \n",
    "# for the source control repository during this workshop. \n",
    "\n",
    "# UPDATE: Add new remote for CodeCommit so we can push code and trigger our pipeline    \n",
    "# Example: https://git-codecommit.us-east-1.amazonaws.com/v1/repos/mlops-codecommit-byo-jdd\n",
    "#!git remote add codecommit https://git-codecommit.us-east-1.amazonaws.com/v1/repos/mlops-codecommit-byo-jdd\n",
    "\n",
    "# Ensure remote repo is now setup\n",
    "!git remote -v\n",
    "\n",
    "!git config --global user.name \"Jane Smith\"\n",
    "!git config --global user.email JaneSmith@example.com\n",
    "\n",
    "!git remote add codecommit https://git-codecommit.us-east-1.amazonaws.com/v1/repos/mlops-codecommit-byo-jdd\n",
    "    \n",
    "!git remote -v\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit training code to the CodeCommit repository to trigger the execution of the CodePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authenticity of host 'github.com (140.82.113.4)' can't be established.\r\n",
      "RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.\r\n",
      "RSA key fingerprint is MD5:16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.\r\n",
      "Are you sure you want to continue connecting (yes/no)? "
     ]
    }
   ],
   "source": [
    "!git pull\n",
    "!git add ./model-code/*\n",
    "!git commit -m \"Initial add of model code to CodeCommit Repo\"\n",
    "!git push codecommit master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor CodePipeline Execution\n",
    "\n",
    "The code above will trigger the execution of your CodePipeline. You can monitor progress of the pipeline execution in the [CodePipeline dashboard](https://console.aws.amazon.com/codesuite/codepipeline/pipelines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

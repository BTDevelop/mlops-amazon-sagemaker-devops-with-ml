{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Environment for BIA Pipeline\n",
    "\n",
    "This notebook instance will act as the lab environment for setting up and triggering changes to our pipeline.  This is being used to provide a consistent environment, gain some familiarity with Amazon SageMaker Notebook Instances, and to avoid any issues with debugging individual laptop configurations during the workshop. \n",
    "\n",
    "PLEASE review the sample notebook [xgboost_customer_churn](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_applying_machine_learning/xgboost_customer_churn) for detailed documentation on the model being built\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  View the Data\n",
    "\n",
    "In this step we are going to upload the data that was processed using the same processing detailed in the  example notebook, [xgboost_customer_churn](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_applying_machine_learning/xgboost_customer_churn).  \n",
    "\n",
    "The following sample shows the label and a subset of the features included in the training dataset.  The label is in the first column, churn, which is the value we are trying to predict to determine whether a customer will churn. \n",
    "\n",
    "[Sample with Header](images/training_data_sample.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Data\n",
      "       0  106  0.1  274.4  120  198.6   82  160.8   62   6.0  3  1  0.2  0.3  \\\n",
      "0     0   28    0  187.8   94  248.6   86  208.8  124  10.6  5  0    0    0   \n",
      "1     1  148    0  279.3  104  201.6   87  280.8   99   7.9  2  2    0    0   \n",
      "...  ..  ...  ...    ...  ...    ...  ...    ...  ...   ... .. ..  ...  ...   \n",
      "2330  0  159    0  198.8  107  195.5   91  213.3  120  16.5  7  5    0    0   \n",
      "2331  0   99   33  179.1   93  238.3  102  165.7   96  10.6  1  2    0    0   \n",
      "\n",
      "      0.4  0.5  0.6  0.7  0.8  0.9  0.10  0.11  0.12  0.13  0.14  1.1  0.15  \\\n",
      "0       0    0    0    0    0    0     0     0     0     0     0    0     0   \n",
      "1       0    0    0    0    0    0     0     0     0     0     0    0     0   \n",
      "...   ...  ...  ...  ...  ...  ...   ...   ...   ...   ...   ...  ...   ...   \n",
      "2330    0    0    0    0    0    0     0     0     0     0     0    0     0   \n",
      "2331    0    0    0    0    0    0     0     0     0     0     0    0     0   \n",
      "\n",
      "      0.16  0.17  0.18  0.19  0.20  0.21  0.22  0.23  0.24  0.25  0.26  0.27  \\\n",
      "0        0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "1        0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
      "2330     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "2331     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "      0.28  0.29  0.30  0.31  0.32  0.33  0.34  0.35  0.36  0.37  0.38  0.39  \\\n",
      "0        0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "1        0     0     0     0     0     0     1     0     0     0     0     0   \n",
      "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
      "2330     0     0     0     0     0     0     0     0     0     1     0     0   \n",
      "2331     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "      0.40  0.41  0.42  0.43  0.44  0.45  0.46  0.47  0.48  0.49  0.50  0.51  \\\n",
      "0        0     0     0     0     0     0     0     0     0     0     0     1   \n",
      "1        0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
      "2330     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "2331     0     1     0     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "      0.52  0.53  1.2  1.3  0.54  1.4  0.55  \n",
      "0        0     1    0    1     0    1     0  \n",
      "1        0     1    0    1     0    1     0  \n",
      "...    ...   ...  ...  ...   ...  ...   ...  \n",
      "2330     0     0    1    1     0    1     0  \n",
      "2331     0     1    0    1     0    0     1  \n",
      "\n",
      "[2332 rows x 70 columns]\n",
      "\n",
      "Smoke Test Data\n",
      "    0  138  0.1  127.1  102  247.7  106  207.7   75   5.0  3  3.1  0.2  0.3  \\\n",
      "0  0  120    0  252.0  120  150.2  106  151.8   96   9.6  1    2    0    0   \n",
      "1  0  112   30   60.6  113  165.9   96  132.8   99  13.3  7    0    0    0   \n",
      "2  0   70    0  197.3   91  305.8   81  171.0  105   6.7  6    1    0    0   \n",
      "3  0   81   46  168.3  124  270.9  103  222.5   98   6.7  2    4    0    0   \n",
      "\n",
      "   0.4  0.5  0.6  0.7  0.8  0.9  0.10  0.11  0.12  0.13  0.14  0.15  0.16  \\\n",
      "0    0    0    0    0    0    0     0     0     0     0     0     0     0   \n",
      "1    0    0    0    0    0    0     0     0     0     0     0     0     0   \n",
      "2    0    0    0    0    0    0     0     0     0     1     0     0     0   \n",
      "3    0    0    0    0    0    0     0     0     0     0     0     0     0   \n",
      "\n",
      "   0.17  0.18  0.19  0.20  0.21  0.22  0.23  0.24  0.25  0.26  0.27  0.28  \\\n",
      "0     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "1     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "2     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "3     1     0     0     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "   0.29  0.30  0.31  0.32  0.33  0.34  0.35  0.36  0.37  0.38  0.39  0.40  \\\n",
      "0     0     0     0     0     0     0     0     0     0     0     1     0   \n",
      "1     0     0     0     0     0     0     0     1     0     0     0     0   \n",
      "2     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "3     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "   0.41  0.42  1  0.43  0.44  0.45  0.46  0.47  0.48  0.49  0.50  0.51  0.52  \\\n",
      "0     0     0  0     0     0     0     0     0     0     0     0     0     0   \n",
      "1     0     0  0     0     0     0     0     0     0     0     0     0     1   \n",
      "2     0     0  0     0     0     0     0     0     0     0     0     0     0   \n",
      "3     0     0  0     0     0     0     0     0     0     0     0     0     1   \n",
      "\n",
      "   0.53  1.1  1.2  0.54  1.3  0.55  \n",
      "0     1    0    1     0    1     0  \n",
      "1     0    0    1     0    0     1  \n",
      "2     1    0    1     0    1     0  \n",
      "3     0    0    1     0    0     1  \n",
      "\n",
      "Validation Data\n",
      "        0   47  28  141.3   94  168.0  108  113.5   84   7.8  2  1  0.1  1.1  \\\n",
      "0    0.0   30   0  247.4  107  175.9   76  287.4   90  11.3  2  0    0    0   \n",
      "1    0.0  106  32  165.9  126  216.5   93  173.1   86  14.1  8  4    0    0   \n",
      "..   ...  ...  ..    ...  ...    ...  ...    ...  ...   ... .. ..  ...  ...   \n",
      "663  0.0   70   0  197.3   91  305.8   81  171.0  105   6.7  6  1    0    0   \n",
      "664  0.0   81  46  168.3  124  270.9  103  222.5   98   6.7  2  4    0    0   \n",
      "\n",
      "     0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  0.10  0.11  0.12  0.13  0.14  \\\n",
      "0      0    0    0    0    0    0    0    0     0     0     0     0     0   \n",
      "1      0    0    0    0    0    0    0    1     0     0     0     0     0   \n",
      "..   ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   ...   ...   ...   \n",
      "663    0    0    0    0    0    0    0    0     0     1     0     0     0   \n",
      "664    0    0    0    0    0    0    0    0     0     0     0     0     0   \n",
      "\n",
      "     0.15  0.16  0.17  0.18  0.19  0.20  0.21  0.22  0.23  0.24  0.25  0.26  \\\n",
      "0       0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "1       0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
      "663     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "664     1     0     0     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "     0.27  0.28  0.29  0.30  0.31  0.32  0.33  0.34  0.35  0.36  0.37  0.38  \\\n",
      "0       0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "1       0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
      "663     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "664     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "     0.39  0.40  0.41  0.42  0.43  0.44  0.45  0.46  0.47  0.48  0.49  0.50  \\\n",
      "0       0     0     1     0     0     0     0     0     0     0     0     0   \n",
      "1       0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
      "663     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "664     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "     1.2  0.51  0.52  1.3  0.53  0.54  1.4  \n",
      "0      0     1     0    1     0     1    0  \n",
      "1      1     0     0    1     0     0    1  \n",
      "..   ...   ...   ...  ...   ...   ...  ...  \n",
      "663    0     1     0    1     0     1    0  \n",
      "664    1     0     0    1     0     0    1  \n",
      "\n",
      "[665 rows x 70 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('./data/train.csv', sep=',')\n",
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 5)         # Keep the output on one page\n",
    "print('\\nTraining Data\\n', train_data)\n",
    "\n",
    "smoketest_data = pd.read_csv('./data/smoketest.csv', sep=',')\n",
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 5)         # Keep the output on one page\n",
    "print('\\nSmoke Test Data\\n', smoketest_data)\n",
    "\n",
    "validation_data = pd.read_csv('./data/validation.csv', sep=',')\n",
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 5)         # Keep the output on one page\n",
    "print('\\nValidation Data\\n', validation_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2:  Upload Data to S3 \n",
    "\n",
    "We will utilize this notebook to perform some of the setup that will be required to trigger the first execution of our pipeline.  In this step, we are going to simulate what would typically be the last step in an Analytics pipeline of creating training and validation datasets. \n",
    "\n",
    "To accomplish this, we will actually be uploading data from our local notebook instance (data can be found under /data/*) to S3.  In a typical scenario, this would be done through your analytics pipeline.  We will use the S3 bucket that was created through the CloudFormation template we launched at the beginning of the lab. You can validate the S3 bucket exists by:\n",
    "  1. Going to the [S3 Service](https://s3.console.aws.amazon.com/s3/) inside the AWS Console\n",
    "  2. Find the name of the S3 data bucket created by the CloudFormation template: mlops-bia- data-*yourintials*-*randomid*\n",
    "  3. Update the bucket variable in the cell below\n",
    "\n",
    "   ### UPDATE THE BUCKET NAME BELOW BEFORE EXECUTING THE CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import time\n",
    "\n",
    "# UPDATE THE NAME OF THE BUCKET TO MATCH THE ONE WE CREATED THROUGH THE CLOUDFORMATION TEMPLATE\n",
    "# Example: mlops-bia-data-jdd-df4d4850\n",
    "#bucket = 'mlops-bia-data-<yourinitials>-<generated id>'\n",
    "bucket = 'mlops-bia-data-jdd-2ba1f4f0'\n",
    "\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "\n",
    "trainfilename = 'train/train.csv'\n",
    "smoketestfilename = 'smoketest/smoketest.csv'\n",
    "validationfilename = 'validation/validation.csv'\n",
    "\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "s3.meta.client.upload_file('./data/train.csv', bucket, trainfilename)\n",
    "s3.meta.client.upload_file('./data/smoketest.csv', bucket, smoketestfilename)\n",
    "s3.meta.client.upload_file('./data/validation.csv', bucket, validationfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 3:  Monitor CodePipeline Execution\n",
    "\n",
    "The code above will trigger the execution of your CodePipeline. You can monitor progress of the pipeline execution in the [CodePipeline dashboard](https://console.aws.amazon.com/codesuite/codepipeline/pipelines).\n",
    "\n",
    "As the pipeline is executing information is being logged to [Cloudwatch logs](https://console.aws.amazon.com/cloudwatch/logs).  Explore the logs for your Lambda functions (/aws/lambda/MLOps-BIA*) as well as output logs from SageMaker (/aws/sagemaker/*). \n",
    "\n",
    "\n",
    "Note: It will take awhile to execute all the way through the pipeline.  Please don't proceed to the next step until the last stage is shows **'succeeded'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Step 5: Edit CodePipeline Triggers to add model retraining\n",
    "\n",
    "In the steps above, we demonstrated how you can trigger the pipeline when new code is committed to CodeCommit.  As you continue to modify your training/inference code, you essentially re-execute the cell above to commit new code and trigger another execution of the pipeline.  Although we are using a notebook instance for the purposes of the workshop, the commit to a source code repository above can happen in your local environment and/or IDEs of choice. \n",
    "\n",
    "In this step, we want to modify the pipeline to add the capability to not only trigger based off code changes but to also trigger a retraining cycle in the event of receiving new training data. If the file/object containing the training data is a single object that will be inclusive of all training data you want to use, you can trigger CodePipeline based on the object itself. However, if you are looking to training incrementally or if your analytics pipeline puts new data to your S3 bucket as deltas then you need to have a trigger mechanism included in your analytics pipeline that notifies CodePipeline it is time to retrain. \n",
    "\n",
    "To simplify the setup for this workshop, we are going to trigger based on a new training dataset that is inclusive of all the data we want to retrain with. \n",
    "\n",
    "   1. Go to your [CodePipeline Pipeline](https://console.aws.amazon.com/codesuite/codepipeline/pipelines) \n",
    "\n",
    "   2. Click on the link to your pipeline (i.e. MLOps-BYO-BuildPipeline*)\n",
    "   \n",
    "   3. Click the **Edit** Button \n",
    "   \n",
    "   4. Inside **Edit:Source** , click the **Edit stage** button --> then Click **Add Action**\n",
    "   \n",
    "   5. Under Edit Action:\n",
    "   \n",
    "       * **Action Name** : RetrainData\n",
    "       \n",
    "       * **Action Provider** : Source - S3\n",
    "       \n",
    "       * **Bucket** : *Enter the name of your S3 data bucket* Example: mlops-data-jdd-d4d740c0\n",
    "       \n",
    "       * **S3 object key** : train/train.csv\n",
    "       \n",
    "       * **Output Artifacts** : RetrainDataIn\n",
    "       \n",
    "       \n",
    "   6. Validate your screen contains all information as shown below: \n",
    "   \n",
    "       \n",
    "       \n",
    "   7. Click **Done**\n",
    "   \n",
    "   8. Click the orange **Save** button in the upper right hand corner to save your changes, confirm changes and hit **Save** again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we may not want rebuild the training/inference container image in the case where we only want to retrain the model, you could optionally create a separate retraining pipeline that excludes the rebuild of the image.  Depending on what you are using for orchestration across your pipeline, you can accomplish this through a single or multiple pipelines. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 6: Trigger retraining based on new data\n",
    " \n",
    "Let's test our new CodePipeline trigger by adding new training data to our S3 data bucket.  The S3 data bucket is setup with versioning enabled.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfilename = 'train/train.csv'\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "s3.meta.client.upload_file('./data/1-train/train/iris-2.csv', bucket, trainfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the cell above, you will see a new trigger for pipeline execution when you click on the link to your [pipeline](https://console.aws.amazon.com/codesuite/codepipeline/pipelines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Clean-Up\n",
    "\n",
    "\n",
    "Return to the [README.md](https://github.com/aws-samples/amazon-sagemaker-devops-with-ml/2-Bring-Your-Own/README.md) to complete the environment cleanup instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONGRATULATIONS! \n",
    "\n",
    "You've built a basic pipeline for the use case of bringing your own algorithm/training/inference code to SageMaker.  This pipeline can act as a starting point for building in additional quality gates such as container scans, manual approvals, and additional evaluation/logging capabilities.   Another common extension to the pipeline may be creating/updating your API serving predictions through API Gateway.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
